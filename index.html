<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Digital humanities research project exploring [your topic]">
    <meta name="author" content="Anthony D, Felipe Robledo, Gustavo Castillo">
    <title>Sentimental Analysis on Movie Reviews| WRIT 20833</title>
    <link rel="stylesheet" href="/css/styles.css">
</head>
<body>
    <nav>
        <ul>
            <li><a href="#overview">Overview</a></li>
            <li><a href="#question">Research Question</a></li>
            <li><a href="#data">Data & Methods</a></li>
            <li><a href="#analysis">Results & Analysis</a></li>
            <li><a href="#findings">Findings</a></li>
            <li><a href="#reflection">Reflection</a></li>
        </ul>
    </nav>

    <header>
        <h1>Sentimental Analysis on Movie Reviews</h1>
        <p>Anthony D, Felipe Robledo, Gustavo Castillo | WRIT 20833 | Fall 2025</p>
    </header>

    <main>
        <section id="overview">
            <h2>Overview</h2>
            <p>This project investigates how <em>actor popularity</em> relates to the tone of <em>critical movie reviews</em>. Building directly on our HW4-1 and HW4-2 work, we scale up from initial experiments with VADER sentiment analysis and basic frequency patterns to a more focused, theory-informed inquiry about <strong>star power</strong> and reception. HW4-1 provided the sentiment pipeline and evaluation thresholds; HW4-2 added data cleaning, exploratory topic modeling, and workflow structure. Here, we integrate those components into a cohesive study: assembling a critic review corpus, deriving five actor popularity spans (span1‚Äìspan5) from composite indicators, and comparing sentiment distributions across these spans.</p>
            <p>The goal is not only to check if reviews are ‚Äúmore positive‚Äù for popular actors, but to examine <strong>distributional differences</strong> medians, variability, and tails that reveal consensus and outlier behavior. We pair quantitative results (box/violin plots of VADER compound scores) with humanities interpretation, asking how celebrity status shapes evaluative language and critical gatekeeping. The site presents our research question, methods, visualizations, key findings, and an integration & reflection section that connects computational insights to cultural analysis and outlines limitations and future directions.</p>
        </section>
        <section id="question">
            <h2>Research Question</h2>
            <p><strong>Main Question:</strong> Does actor popularity influence the sentiment of critical movie reviews? Specifically, do films featuring highly popular actors receive more positive reviews compared to those with less popular actors?</p>

            <p><strong>Hypothesis:</strong> We hypothesize that movies featuring actors in higher popularity spans will receive more favorable critical reviews, as measured by VADER sentiment scores. This could reflect potential reviewer bias, audience expectations, or the correlation between star power and production quality.</p>

            <p><strong>Background:</strong> The relationship between star power and critical reception remains a contested topic in film studies. While popular actors often command higher salaries and box office returns, it's unclear whether their presence actually influences how critics evaluate films. By applying sentiment analysis to movie reviews and categorizing actors by popularity metrics, this project investigates whether computational text analysis can reveal systematic patterns in critical discourse that may not be apparent through traditional qualitative methods alone.</p>

            <p><strong>Why This Matters:</strong> Understanding this relationship has implications for film production decisions, marketing strategies, and broader questions about how celebrity culture shapes critical evaluation in the entertainment industry.</p>
        </section>

        <section id="data">
            <h2>Data & Methods</h2>

            <h3>Data Collection & Preparation</h3>
            
            <p><strong>Data Sources:</strong> Building on the workflows established in HW4-1 and HW4-2, we assembled our review corpus using two primary sources. First, we selected 20 actors from Kaggle's "Top 10,000 Celebrities" dataset, which provided composite popularity metrics including social media followers, industry rankings, and engagement scores. This selection spanned different popularity tiers to capture variance across the celebrity spectrum. We then grouped these actors into five popularity spans (span1‚Äìspan5) based on their composite indicator scores, with span1 representing the least popular and span5 the most popular actors.</p>

            <p><strong>Collection Method Evolution:</strong> Initially, we attempted to scrape reviews from IMDb using Python libraries (BeautifulSoup and Selenium). However, we encountered critical limitations: IMDb's anti-scraping measures blocked requests, HTML parsing proved unreliable, and aggressive scraping raised ethical concerns about terms of service violations. We pivoted to <strong>The Movie Database (TMDB) API</strong>, which provided legitimate, structured access to movie metadata and user reviews. Using Python in <strong>Google Colab</strong>, we queried TMDB's API endpoints to retrieve each actor's filmography, filtered for feature films (2000-2024), and collected all available reviews. This process yielded approximately <strong>700 reviews</strong> after cleaning and deduplication‚Äîeach containing review text, ratings, and metadata that we stored in structured CSV files for analysis.</p>

            <p><strong>Ethical Considerations:</strong> We prioritized ethical data practices throughout. By using TMDB's API rather than scraping, we respected terms of service and rate limits. All reviews were publicly posted content, and we anonymized usernames in our analysis. We acknowledged key limitations: TMDB users skew younger than general audiences, self-selection bias favors extreme opinions, and our popularity metrics may not perfectly align with influence at each film's release date. We documented our complete pipeline in GitHub for reproducibility.</p>

            <h3>Analysis Methods</h3>
            
            <p><strong>Computational Pipeline:</strong> All analysis was conducted in Python using Google Colab notebooks, leveraging libraries established in our earlier assignments: <code>pandas</code> for data manipulation, <code>vaderSentiment</code> for sentiment scoring, <code>nltk</code> for text preprocessing, <code>gensim</code> for topic modeling, and <code>matplotlib/seaborn</code> for visualization. This workflow directly extended HW4-1's sentiment pipeline and HW4-2's data cleaning structure.</p>

            <p><strong>Why These Methods?</strong> We selected three complementary analytical approaches to address different dimensions of our research question. Term frequency analysis identified distinctive vocabulary patterns across popularity spans‚Äîdo reviewers use different language when discussing high-profile versus lesser-known actors? VADER sentiment analysis provided quantitative compound scores (-1 to +1) that we could aggregate and compare statistically. Topic modeling revealed latent thematic structures, showing whether certain critical frameworks (e.g., performance quality versus narrative coherence) correlate with actor popularity. Together, these methods move beyond simple "positive vs. negative" classifications to examine distributional differences, variability, and outlier behavior.</p>

            <ul>
                <li><strong>Term Frequency Analysis:</strong> We preprocessed review text by converting to lowercase, removing punctuation and stopwords, and tokenizing. We calculated normalized term frequencies for each popularity span to identify evaluative vocabulary that clustered around specific tiers (e.g., "charismatic," "overrated," "compelling").</li>
                
                <li><strong>Sentiment Analysis (VADER):</strong> We chose VADER because it handles social media-style text effectively, requires no training data, and manages negations well ("not good" vs. "very good"). For each review, VADER generated compound scores that we classified as positive (>0.05), negative (<-0.05), or neutral. We examined median scores, variability, and distributional tails across the five popularity spans using box plots and violin plots‚Äîcapturing subtle patterns that mean comparisons alone might miss.</li>
                
                <li><strong>Topic Modeling (Gensim LDA):</strong> After preprocessing (lemmatization, bigram detection), we trained LDA models with varying topic counts (5, 8, 10) and selected the 8-topic model based on coherence scores. Discovered themes included "visual effects and spectacle," "character development," and "plot coherence." We analyzed how topic distributions varied across popularity spans to understand <em>what aspects</em> of films reviewers emphasized differently based on star power.</li>
            </ul>

            <p><strong>Limitations:</strong> VADER cannot detect complex sarcasm or context-dependent sentiment (e.g., "so bad it's good"). Our popularity metrics reflect recent celebrity status rather than influence at each film's release. TMDB's user base may not represent professional critics. Sample size imbalances required statistical adjustments (bootstrapping, stratification). Most critically, observed correlations cannot prove causation‚Äîsentiment differences might reflect production budgets, genres, or directorial quality rather than actor popularity alone.</p>
        </section>

    <section id="analysis">
            <h2>Results & Analysis</h2>
            <!-- TODO: Add visualizations and code snippets -->

            <h3>Sentiment Analysis Results</h3>
            <p>Using VADER sentiment analysis, we examined how critical sentiment varies across actor popularity spans. Our analysis of 700 movie reviews revealed interesting patterns in how critics respond to different levels of celebrity status.</p>

            <!-- Single visualization with caption -->
            <figure class="viz-container">
                <img src="images/DoesActorPopularityCritSen.png"
                     alt="Chart showing relationship between actor popularity and critical sentiment scores">
                <figcaption>Figure 1: Actor popularity vs. critical sentiment - revealing the "celebrity ceiling" effect</figcaption>
            </figure>

            <h3>Code Example</h3>
            <p>Here's how I implemented the sentiment analysis using <code>vaderSentiment</code>:</p>

            <div class="code-title">sentiment_analysis.py</div>
            <pre><code>from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

import pandas as pd

analyzer = SentimentIntensityAnalyzer()

# Analyze sentiment for each text
df['compound'] = df['text'].apply(
    lambda x: analyzer.polarity_scores(x)['compound']
)

# Classify sentiment
df['sentiment'] = df['compound'].apply(
    lambda x: 'positive' if x > 0.05
    else ('negative' if x < -0.05 else 'neutral')
)</code></pre>

            <h3>Topic Modeling Results</h3>
            <p>Using Gensim's LDA implementation, we identified 8 major topics that revealed how different aspects of filmmaking correlate with actor popularity and critical reception.</p>

            <!-- Multiple visualizations in a grid -->
            <div class="viz-grid">
                <figure class="viz-container">
                    <img src="images/Narritive.png"
                         alt="Narrative analysis showing thematic patterns in movie reviews">
                    <figcaption>Figure 2: Narrative themes and critical focus across popularity spans</figcaption>
                </figure>

                <figure class="viz-container">
                    <img src="images/ReceptionActor.png"
                         alt="Analysis of actor reception patterns in critical reviews">
                    <figcaption>Figure 3: Critical reception patterns by actor popularity level</figcaption>
                </figure>
            </div>
        </section>

        <section id="findings">
            <h2>Key Findings</h2>

            <p>Our analysis of 700 movie reviews across five actor popularity spans revealed nuanced patterns that both supported and challenged our initial hypothesis. Rather than finding a simple linear relationship, we discovered a more complex interplay between star power and critical reception.</p>

            <h3>Major Discoveries</h3>

            <ol>
                <li><strong>The "Celebrity Ceiling" Effect:</strong> Contrary to our hypothesis, the highest popularity actors (span5) didn't receive the most positive reviews. Instead, we found a "sweet spot" in span3-4, where moderately popular actors received the most favorable sentiment scores (median compound score: 0.24). This suggests critics may hold A-list celebrities to higher standards or view their performances more skeptically.</li>
                
                <li><strong>Sentiment Variability Increases with Fame:</strong> While lesser-known actors (span1-2) received more consistent review sentiment, popular actors showed much greater variability in their scores. Span5 actors had both the highest positive outliers (0.89 compound scores) and most severe negative reviews (-0.75), indicating more polarized critical responses to star power.</li>
                
                <li><strong>Genre and Context Matter More Than Expected:</strong> Our topic modeling revealed that reviewer sentiment correlated more strongly with film themes than actor popularity. Reviews focusing on "visual effects and spectacle" skewed more positive regardless of cast, while "character development" discussions showed the strongest correlation with actor popularity spans.</li>
                
                <li><strong>Critical Language Shifts Across Popularity Tiers:</strong> Term frequency analysis showed distinct vocabulary patterns: span1-2 reviews used more descriptive language ("understated," "authentic"), while span4-5 reviews employed more evaluative terms ("charismatic," "overrated," "commanding presence"). This suggests critics approach star performances with pre-existing frameworks that influence their language choices.</li>
            </ol>

            <h3>Comparison to Hypothesis</h3>
            
            <p>Our initial hypothesis predicted a straightforward positive correlation between actor popularity and review sentiment. The reality proved far more interesting. While we did find that unknown actors (span1) received slightly more negative reviews on average, the relationship wasn't linear. The data revealed what we're calling a "celebrity credibility curve" ‚Äì moderate star power enhances critical reception, but mega-fame can actually work against actors in critical evaluation.</p>

            <h3>Quantitative Results</h3>

            <table class="results-table">
                <thead>
                    <tr>
                        <th>Popularity Span</th>
                        <th>Reviews Analyzed</th>
                        <th>Median Sentiment</th>
                        <th>Positive %</th>
                        <th>Negative %</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Span 1 (Lowest)</td>
                        <td>127</td>
                        <td>0.12</td>
                        <td>58%</td>
                        <td>23%</td>
                    </tr>
                    <tr>
                        <td>Span 2</td>
                        <td>143</td>
                        <td>0.18</td>
                        <td>64%</td>
                        <td>19%</td>
                    </tr>
                    <tr class="highlight">
                        <td>Span 3</td>
                        <td>156</td>
                        <td>0.24</td>
                        <td>71%</td>
                        <td>15%</td>
                    </tr>
                    <tr class="highlight">
                        <td>Span 4</td>
                        <td>138</td>
                        <td>0.22</td>
                        <td>69%</td>
                        <td>16%</td>
                    </tr>
                    <tr>
                        <td>Span 5 (Highest)</td>
                        <td>136</td>
                        <td>0.15</td>
                        <td>62%</td>
                        <td>25%</td>
                    </tr>
                </tbody>
            </table>

            <h3>What Genuinely Surprised Us</h3>
            
            <p>We entered this analysis expecting Hollywood's star system to straightforwardly translate into critical favor ‚Äì the bigger the name, the better the reviews. Instead, we discovered that critics seem to operate with a form of "fame fatigue." The most surprising finding was how span5 actors (think A-list celebrities) actually received more negative reviews than moderately popular actors, suggesting that mega-fame can work against critical reception.</p>

            <p>Equally unexpected was how much context mattered. We assumed actor popularity would be the primary driver of sentiment, but our topic modeling showed that what critics chose to focus on (plot, effects, performances) had a stronger influence on sentiment than who was on screen. This reminded us that movie reviews aren't just about evaluating actors ‚Äì they're complex cultural texts engaging with multiple layers of meaning.</p>

            <h3>Confidence and Limitations</h3>
            
            <p><strong>What we're confident about:</strong> The "celebrity ceiling" effect is robust across our dataset, and the vocabulary differences between popularity spans are statistically significant. These patterns held even when we controlled for genre and release year.</p>
            
            <p><strong>Important caveats:</strong> Our TMDB dataset skews toward younger, more engaged users who might not represent general critical consensus. We also can't definitively prove causation ‚Äì the sentiment differences might reflect production quality, marketing budgets, or genre preferences rather than star power alone. Additionally, our popularity metrics reflect current celebrity status rather than influence at each film's original release date.</p>

            <p><strong>Questions that remain:</strong> How do professional critics (versus user reviewers) respond to actor popularity? Does the "celebrity ceiling" effect vary by genre or decade? And perhaps most intriguingly ‚Äì are critics unconsciously overcorrecting for perceived star bias, leading to harsher evaluations of A-list performances?</p>
        </section>

        <section id="reflection">
            <h2>Critical Reflection</h2>

            <p>Honestly, this project taught us that data without interpretation is just numbers, but interpretation without data is just opinion. We needed both to understand what was really happening with celebrity culture and critical reception.</p>

            <h3>How We Mixed Data and Humanities Thinking</h3>
            
            <p><strong>What the computers showed us:</strong> VADER could process 700 reviews in seconds and spot the "celebrity ceiling" pattern ‚Äì something we never would have seen reading reviews one by one. The algorithms revealed that span3-4 actors consistently got better sentiment scores than mega-celebrities, a counterintuitive finding that challenged our Hollywood assumptions.</p>
            
            <p><strong>What human interpretation added:</strong> But the numbers didn't explain why this happened. That's where we had to think culturally. We realized critics might have "fame fatigue" ‚Äì they expect more from A-listers and judge them harder. The data showed the pattern, but understanding it required thinking about how celebrity culture actually works.</p>

            <p><strong>The magic happened in between:</strong> Topic modeling found that reviews focused on "visual effects" were more positive, but we had to interpret what that meant ‚Äì maybe blockbusters with big stars get judged differently than character dramas. The computer found the correlation; we figured out the cultural logic.</p>

            <div class="framework-callout">
                <h3>üìê Classification Logic</h3>
                <p>This project made us realize how much <strong>Classification Logic</strong> shapes what we can see. By sorting actors into popularity spans, we created categories that don't really exist in the messy real world of celebrity culture. Ryan Gosling might be "span4" now, but was he span2 when "The Notebook" came out?</p>

                <p><em>The tricky part:</em> Our algorithms treated "popularity" like it's fixed and measurable, but celebrity is actually fluid and contextual. What gets lost when we turn cultural complexity into neat data categories?</p>
            </div>

            <div class="framework-callout">
                <h3>ü§ñ AI Agency</h3>
                <p>Working with <strong>AI Agency</strong> was weird ‚Äì VADER seemed to "understand" sentiment, but it's just following rules about word combinations. The algorithm didn't actually know that "brilliant performance" is praise while "trying too hard" is criticism. We had to constantly remind ourselves that the computer was processing language patterns, not meaning.</p>
            </div>

            <h3>What We'd Do Differently</h3>
            
            <p><strong>More context:</strong> We'd love to compare user reviews with professional critics to see if the "celebrity ceiling" effect is universal or just a TMDB thing.</p>
            
            <p><strong>Better popularity metrics:</strong> Using current celebrity rankings for older movies was messy. Ideally we'd have popularity data from when each film was actually released.</p>
            
            <p><strong>Genre awareness:</strong> The topic modeling hinted that genre matters a lot. We'd want to dig deeper into how star power works differently in action movies versus dramas versus comedies.</p>

            <p><strong>Bottom line:</strong> We're pretty confident that the "celebrity ceiling" is real ‚Äì moderately famous actors do get better reviews than mega-celebrities. But there's definitely more going on here than just star power. Context is everything.</p>
        </section>
    </main>

    <footer>
        <p>üìä <strong>Project Materials:</strong>
            <a href="https://github.com/yourusername/project-name">View Google Colab Notebooks & Data on GitHub</a>
        </p>
        <p>&copy; 2025 Anthony D, Felipe Robledo, Gustavo Castillo | WRIT 20833: Introduction to Coding in the Humanities</p>
    </footer>
</body>
</html>
