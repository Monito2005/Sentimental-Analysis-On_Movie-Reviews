<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Digital humanities research project exploring [your topic]">
    <meta name="author" content="Anthony D, Felipe Robledo, Gustavo Castillo">
    <title>Sentimental Analysis on Movie Reviews| WRIT 20833</title>
    <link rel="stylesheet" href="/css/styles.css">
</head>
<body>
    <nav>
        <ul>
            <li><a href="#overview">Overview</a></li>
            <li><a href="#question">Research Question</a></li>
            <li><a href="#data">Data & Methods</a></li>
            <li><a href="#analysis">Results & Analysis</a></li>
            <li><a href="#findings">Findings</a></li>
            <li><a href="#reflection">Reflection</a></li>
        </ul>
    </nav>

    <header>
        <h1>Sentimental Analysis on Movie Reviews</h1>
        <p>Anthony D, Felipe Robledo, Gustavo Castillo | WRIT 20833 | Fall 2025</p>
    </header>

    <main>
        <section id="overview">
            <h2>Overview</h2>
            <p>This project investigates how <em>actor popularity</em> relates to the tone of <em>critical movie reviews</em>. Building directly on our HW4-1 and HW4-2 work, we scale up from initial experiments with VADER sentiment analysis and basic frequency patterns to a more focused, theory-informed inquiry about <strong>star power</strong> and reception. HW4-1 provided the sentiment pipeline and evaluation thresholds; HW4-2 added data cleaning, exploratory topic modeling, and workflow structure. Here, we integrate those components into a cohesive study: assembling a critic review corpus, deriving five actor popularity spans (span1‚Äìspan5) from composite indicators, and comparing sentiment distributions across these spans.</p>
            <p>The goal is not only to check if reviews are ‚Äúmore positive‚Äù for popular actors, but to examine <strong>distributional differences</strong> medians, variability, and tails that reveal consensus and outlier behavior. We pair quantitative results (box/violin plots of VADER compound scores) with humanities interpretation, asking how celebrity status shapes evaluative language and critical gatekeeping. The site presents our research question, methods, visualizations, key findings, and an integration & reflection section that connects computational insights to cultural analysis and outlines limitations and future directions.</p>
        </section>
        <section id="question">
            <h2>Research Question</h2>
            <p><strong>Main Question:</strong> Does actor popularity influence the sentiment of critical movie reviews? Specifically, do films featuring highly popular actors receive more positive reviews compared to those with less popular actors?</p>

            <p><strong>Hypothesis:</strong> We hypothesize that movies featuring actors in higher popularity spans will receive more favorable critical reviews, as measured by VADER sentiment scores. This could reflect potential reviewer bias, audience expectations, or the correlation between star power and production quality.</p>

            <p><strong>Background:</strong> The relationship between star power and critical reception remains a contested topic in film studies. While popular actors often command higher salaries and box office returns, it's unclear whether their presence actually influences how critics evaluate films. By applying sentiment analysis to movie reviews and categorizing actors by popularity metrics, this project investigates whether computational text analysis can reveal systematic patterns in critical discourse that may not be apparent through traditional qualitative methods alone.</p>

            <p><strong>Why This Matters:</strong> Understanding this relationship has implications for film production decisions, marketing strategies, and broader questions about how celebrity culture shapes critical evaluation in the entertainment industry.</p>
        </section>

        <section id="data">
            <h2>Data & Methods</h2>

            <h3>Data Collection & Preparation</h3>
            
            <p><strong>Data Sources:</strong> Building on the workflows established in HW4-1 and HW4-2, we assembled our review corpus using two primary sources. First, we selected 20 actors from Kaggle's "Top 10,000 Celebrities" dataset, which provided composite popularity metrics including social media followers, industry rankings, and engagement scores. This selection spanned different popularity tiers to capture variance across the celebrity spectrum. We then grouped these actors into five popularity spans (span1‚Äìspan5) based on their composite indicator scores, with span1 representing the least popular and span5 the most popular actors.</p>

            <p><strong>Collection Method Evolution:</strong> Initially, we attempted to scrape reviews from IMDb using Python libraries (BeautifulSoup and Selenium). However, we encountered critical limitations: IMDb's anti-scraping measures blocked requests, HTML parsing proved unreliable, and aggressive scraping raised ethical concerns about terms of service violations. We pivoted to <strong>The Movie Database (TMDB) API</strong>, which provided legitimate, structured access to movie metadata and user reviews. Using Python in <strong>Google Colab</strong>, we queried TMDB's API endpoints to retrieve each actor's filmography, filtered for feature films (2000-2024), and collected all available reviews. This process yielded approximately <strong>700 reviews</strong> after cleaning and deduplication‚Äîeach containing review text, ratings, and metadata that we stored in structured CSV files for analysis.</p>

            <p><strong>Ethical Considerations:</strong> We prioritized ethical data practices throughout. By using TMDB's API rather than scraping, we respected terms of service and rate limits. All reviews were publicly posted content, and we anonymized usernames in our analysis. We acknowledged key limitations: TMDB users skew younger than general audiences, self-selection bias favors extreme opinions, and our popularity metrics may not perfectly align with influence at each film's release date. We documented our complete pipeline in GitHub for reproducibility.</p>

            <h3>Analysis Methods</h3>
            
            <p><strong>Computational Pipeline:</strong> All analysis was conducted in Python using Google Colab notebooks, leveraging libraries established in our earlier assignments: <code>pandas</code> for data manipulation, <code>vaderSentiment</code> for sentiment scoring, <code>nltk</code> for text preprocessing, <code>gensim</code> for topic modeling, and <code>matplotlib/seaborn</code> for visualization. This workflow directly extended HW4-1's sentiment pipeline and HW4-2's data cleaning structure.</p>

            <p><strong>Why These Methods?</strong> We selected three complementary analytical approaches to address different dimensions of our research question. Term frequency analysis identified distinctive vocabulary patterns across popularity spans‚Äîdo reviewers use different language when discussing high-profile versus lesser-known actors? VADER sentiment analysis provided quantitative compound scores (-1 to +1) that we could aggregate and compare statistically. Topic modeling revealed latent thematic structures, showing whether certain critical frameworks (e.g., performance quality versus narrative coherence) correlate with actor popularity. Together, these methods move beyond simple "positive vs. negative" classifications to examine distributional differences, variability, and outlier behavior.</p>

            <ul>
                <li><strong>Term Frequency Analysis:</strong> We preprocessed review text by converting to lowercase, removing punctuation and stopwords, and tokenizing. We calculated normalized term frequencies for each popularity span to identify evaluative vocabulary that clustered around specific tiers (e.g., "charismatic," "overrated," "compelling").</li>
                
                <li><strong>Sentiment Analysis (VADER):</strong> We chose VADER because it handles social media-style text effectively, requires no training data, and manages negations well ("not good" vs. "very good"). For each review, VADER generated compound scores that we classified as positive (>0.05), negative (<-0.05), or neutral. We examined median scores, variability, and distributional tails across the five popularity spans using box plots and violin plots‚Äîcapturing subtle patterns that mean comparisons alone might miss.</li>
                
                <li><strong>Topic Modeling (Gensim LDA):</strong> After preprocessing (lemmatization, bigram detection), we trained LDA models with varying topic counts (5, 8, 10) and selected the 8-topic model based on coherence scores. Discovered themes included "visual effects and spectacle," "character development," and "plot coherence." We analyzed how topic distributions varied across popularity spans to understand <em>what aspects</em> of films reviewers emphasized differently based on star power.</li>
            </ul>

            <p><strong>Limitations:</strong> VADER cannot detect complex sarcasm or context-dependent sentiment (e.g., "so bad it's good"). Our popularity metrics reflect recent celebrity status rather than influence at each film's release. TMDB's user base may not represent professional critics. Sample size imbalances required statistical adjustments (bootstrapping, stratification). Most critically, observed correlations cannot prove causation‚Äîsentiment differences might reflect production budgets, genres, or directorial quality rather than actor popularity alone.</p>
        </section>

    <section id="analysis">
            <h2>Results & Analysis</h2>
            <!-- TODO: Add visualizations and code snippets -->

            <h3>Sentiment Analysis Results</h3>
            <p>Using VADER sentiment analysis, I examined [describe what you analyzed]...</p>

            <!-- Single visualization with caption -->
            <figure class="viz-container">
                <img src="images/sentiment-distribution.png"
                     alt="Bar chart showing distribution of positive, negative, and neutral sentiment">
                <figcaption>Figure 1: Distribution of sentiment scores across dataset</figcaption>
            </figure>

            <h3>Code Example</h3>
            <p>Here's how I implemented the sentiment analysis using <code>vaderSentiment</code>:</p>

            <div class="code-title">sentiment_analysis.py</div>
            <pre><code>from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

import pandas as pd

analyzer = SentimentIntensityAnalyzer()

# Analyze sentiment for each text
df['compound'] = df['text'].apply(
    lambda x: analyzer.polarity_scores(x)['compound']
)

# Classify sentiment
df['sentiment'] = df['compound'].apply(
    lambda x: 'positive' if x > 0.05
    else ('negative' if x < -0.05 else 'neutral')
)</code></pre>

            <h3>Topic Modeling Results</h3>
            <p>Using Gensim's LDA implementation, I identified [number] major topics...</p>

            <!-- Multiple visualizations in a grid -->
            <div class="viz-grid">
                <figure class="viz-container">
                    <img src="images/topic-model.png"
                         alt="Visualization of topic clusters from LDA analysis">
                    <figcaption>Figure 2: Topic clusters from LDA analysis</figcaption>
                </figure>

                <figure class="viz-container">
                    <img src="images/word-cloud.png"
                         alt="Word cloud showing most frequent terms">
                    <figcaption>Figure 3: Most common terms in the corpus</figcaption>
                </figure>
            </div>
        </section>

        <section id="findings">
            <h2>Key Findings</h2>
            <!-- TODO: Present your main discoveries -->

            <p>The computational analysis revealed three major insights:</p>

            <ol>
                <li><strong>Finding 1:</strong> [Describe your first key discovery]</li>
                <li><strong>Finding 2:</strong> [Describe your second key discovery]</li>
                <li><strong>Finding 3:</strong> [Describe your third key discovery]</li>
            </ol>

            <h3>Detailed Results</h3>
            <p>Breaking down the sentiment distribution:</p>

            <table class="results-table">
                <thead>
                    <tr>
                        <th>Sentiment Category</th>
                        <th>Count</th>
                        <th>Percentage</th>
                        <th>Avg. Compound Score</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="highlight">
                        <td>Positive</td>
                        <td>XXX</td>
                        <td>XX%</td>
                        <td>0.XX</td>
                    </tr>
                    <tr>
                        <td>Neutral</td>
                        <td>XXX</td>
                        <td>XX%</td>
                        <td>0.XX</td>
                    </tr>
                    <tr>
                        <td>Negative</td>
                        <td>XXX</td>
                        <td>XX%</td>
                        <td>-0.XX</td>
                    </tr>
                </tbody>
            </table>

            <h3>What Surprised Me</h3>
            <p>I initially predicted that [your assumption], but the data revealed [what actually happened]. This challenged my understanding because...</p>
        </section>

        <section id="reflection">
            <h2>Critical Reflection</h2>
            <!-- TODO: Connect to course frameworks -->

            <p>This project demonstrates what happens when coding meets culture by revealing insights that neither computational analysis nor traditional close reading could discover alone.</p>

            <h3>Integration of Methods</h3>
            <p><strong>What computational methods revealed:</strong> [Describe patterns only visible through large-scale analysis]</p>
            <p><strong>What close reading added:</strong> [Describe how interpretive work enriched the computational findings]</p>

            <div class="framework-callout">
                <h3>üìê Classification Logic</h3>
                <p>This project connects to <strong>Classification Logic</strong> by revealing how algorithmic categorization shapes our understanding of [your topic]. [Explain the connection...]</p>

                <p><em>Critical question:</em> What nuances are lost when we reduce complex cultural expressions to computational categories?</p>
            </div>

            <div class="framework-callout">
                <h3>ü§ñ AI Agency</h3>
                <p>The use of topic modeling and sentiment analysis demonstrates <strong>AI Agency</strong> concerns. While the algorithms appear to "discover" meaning, the interpretation and framing of results remains entirely human. [Explain further...]</p>
            </div>

            <h3>Limitations & Future Directions</h3>
            <p><strong>What I would do differently:</strong> [Reflect on your process]</p>
            <p><strong>Questions that remain:</strong> [What would you investigate with more time?]</p>
            <p><strong>Confidence in conclusions:</strong> [How certain are you about your findings? What caveats should readers consider?]</p>
        </section>
    </main>

    <footer>
        <p>üìä <strong>Project Materials:</strong>
            <a href="https://github.com/yourusername/project-name">View Google Colab Notebooks & Data on GitHub</a>
        </p>
        <p>&copy; 2025 Anthony D, Felipe Robledo, Gustavo Castillo | WRIT 20833: Introduction to Coding in the Humanities</p>
    </footer>
</body>
</html>
