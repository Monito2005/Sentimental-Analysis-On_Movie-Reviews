{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guG7MjYvZFvf"
      },
      "source": [
        "# **Sentimental Analysis on Movie Reviews**\n",
        "**Anthony Do, Felipe Robledo, Gustavo Castillo | WRIT 20833 | Fall 2025**\n",
        "\n",
        "## **Overview**\n",
        "This project investigates how actor popularity relates to the tone of critical movie reviews. Building directly on our HW4-1 and HW4-2 work, we scale up from initial experiments with VADER sentiment analysis and basic frequency patterns to a more focused, theory-informed inquiry about star power and reception. HW4-1 provided the sentiment pipeline and evaluation thresholds; HW4-2 added data cleaning, exploratory topic modeling, and workflow structure. Here, we integrate those components into a cohesive study: assembling a critic review corpus, deriving five actor popularity spans (span1–span5) from composite indicators, and comparing sentiment distributions across these spans.\n",
        "\n",
        "The goal is not only to check if reviews are “more positive” for popular actors, but to examine distributional differences medians, variability, and tails that reveal consensus and outlier behavior. We pair quantitative results (box/violin plots of VADER compound scores) with humanities interpretation, asking how celebrity status shapes evaluative language and critical gatekeeping. The site presents our research question, methods, visualizations, key findings, and an integration & reflection section that connects computational insights to cultural analysis and outlines limitations and future directions.\n",
        "\n",
        "## **Research Question**\n",
        "**Main Question:** Does actor popularity influence the sentiment of critical movie reviews? Specifically, do films featuring highly popular actors receive more positive reviews compared to those with less popular actors?\n",
        "\n",
        "**Hypothesis:** We hypothesize that movies featuring actors in higher popularity spans will receive more favorable critical reviews, as measured by VADER sentiment scores. This could reflect potential reviewer bias, audience expectations, or the correlation between star power and production quality.\n",
        "\n",
        "**Background:** The relationship between star power and critical reception remains a contested topic in film studies. While popular actors often command higher salaries and box office returns, it's unclear whether their presence actually influences how critics evaluate films. By applying sentiment analysis to movie reviews and categorizing actors by popularity metrics, this project investigates whether computational text analysis can reveal systematic patterns in critical discourse that may not be apparent through traditional qualitative methods alone.\n",
        "\n",
        "**Why This Matters:** Understanding this relationship has implications for film production decisions, marketing strategies, and broader questions about how celebrity culture shapes critical evaluation in the entertainment industry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmY6HZfiZFvg"
      },
      "outputs": [],
      "source": [
        "# mounting google drive to access my datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5IqDgf2ZFvg"
      },
      "outputs": [],
      "source": [
        "# installing the wrapper for the movie database api and tqdm for progress bars\n",
        "!pip install tmdbv3api tqdm\n",
        "# installing vader sentiment for the analysis part\n",
        "!pip install vaderSentiment\n",
        "# importing the sentiment analyzer tool\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "# importing pandas to handle tables\n",
        "import pandas as pd\n",
        "import os\n",
        "# importing the specific api modules we need\n",
        "from tmdbv3api import TMDb, Person, Movie\n",
        "from tqdm import tqdm # this is for the progress bar\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46aBW3HOZFvh"
      },
      "outputs": [],
      "source": [
        "# initializing the tmdb api object\n",
        "tmdb = TMDb()\n",
        "# setting up my api key for authentication\n",
        "tmdb.api_key = 'adbbcdff153251b5567ce70c415a5119'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC_pkfkiZFvh"
      },
      "outputs": [],
      "source": [
        "# testing the api connection by checking details for a specific actor id\n",
        "person_api = Person()\n",
        "person_details = person_api.details(2091760)\n",
        "# printing the available keys to see what data we can get\n",
        "display(vars(person_details).keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbeFf6CLZFvh"
      },
      "source": [
        "## **Data & Methods**\n",
        "\n",
        "### **Data Collection & Preparation**\n",
        "**Data Sources:** Building on the workflows established in HW4-1 and HW4-2, we assembled our review corpus using two primary sources. First, we selected 20 actors from Kaggle's \"Top 10,000 Celebrities\" dataset, which provided composite popularity metrics including social media followers, industry rankings, and engagement scores. This selection spanned different popularity tiers to capture variance across the celebrity spectrum. We then grouped these actors into five popularity spans (span1–span5) based on their composite indicator scores, with span1 representing the least popular and span5 the most popular actors.\n",
        "\n",
        "**Collection Method Evolution:** Initially, we attempted to scrape reviews from IMDb using Python libraries (BeautifulSoup and Selenium). However, we encountered critical limitations: IMDb's anti-scraping measures blocked requests, HTML parsing proved unreliable, and aggressive scraping raised ethical concerns about terms of service violations. We pivoted to The Movie Database (TMDB) API, which provided legitimate, structured access to movie metadata and user reviews. Using Python in Google Colab, we queried TMDB's API endpoints to retrieve each actor's filmography, filtered for feature films (2000-2024), and collected all available reviews. This process yielded approximately 700 reviews after cleaning and deduplication—each containing review text, ratings, and metadata that we stored in structured CSV files for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFy7BVUyZFvh"
      },
      "outputs": [],
      "source": [
        "# loading the main celebrity dataset from my drive\n",
        "file_path_3 = '/content/drive/My Drive/WRIT20833/Data/Final_Project/Celebrity.csv'\n",
        "celeb = pd.read_csv(file_path_3)\n",
        "\n",
        "print(f\"Original DataFrame has {len(celeb)} entries.\")\n",
        "\n",
        "# filtering the data to only include people who are known for acting\n",
        "actors_df = celeb[celeb['known_for_department'] == 'Acting'].copy()\n",
        "print(f\"Found {len(actors_df)} actors in the file.\")\n",
        "\n",
        "# sorting the actors by popularity score from high to low\n",
        "actors_df = actors_df.sort_values(by='popularity', ascending=False).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2K-Gf2FZFvh"
      },
      "outputs": [],
      "source": [
        "# defining the index ranges for our 5 popularity groups (spans)\n",
        "span_indices = {\n",
        "    'span1': (0, 100),    # Ranks 1-100 (index 0-99)\n",
        "    'span2': (100, 1000),  # Ranks 101-1000 (index 100-999)\n",
        "    'span3': (1000, 2000), # Ranks 1001-2000 (index 1000-1999)\n",
        "    'span4': (2000, 5000), # Ranks 2001-5000 (index 2000-4999)\n",
        "    'span5': (5000, len(actors_df)) # Ranks 5001+ (index 5000 to end)\n",
        "}\n",
        "\n",
        "# creating a dictionary to store the dataframes for each span\n",
        "spans_data = {}\n",
        "\n",
        "print(\"\\n--- Slicing DataFrame into 5 spans ---\")\n",
        "\n",
        "# selecting only the columns we need for analysis\n",
        "columns_to_keep = ['id', 'name', 'popularity'] # Added 'name' for easier checking\n",
        "\n",
        "# looping through our defined indices to slice the main dataframe\n",
        "for span_name, (start, end) in span_indices.items():\n",
        "    # slicing the main dataframe based on the current range\n",
        "    span_df = actors_df.iloc[start:end]\n",
        "\n",
        "    # filtering for specific columns\n",
        "    span_df = span_df[columns_to_keep].copy()\n",
        "\n",
        "    # saving the slice to our dictionary\n",
        "    spans_data[span_name] = span_df\n",
        "\n",
        "    print(f\"Created {span_name}: {len(span_df)} actors (Ranks {start+1} to {end})\")\n",
        "\n",
        "# --- Now you can access each DataFrame ---\n",
        "print(\"\\n--- Previews of your 5 spans ---\")\n",
        "\n",
        "# checking the first span\n",
        "print(\"\\nSpan 1 (Top 100):\")\n",
        "display(spans_data['span1'].head(3))\n",
        "print(f\"Popularity range: {spans_data['span1']['popularity'].min()} to {spans_data['span1']['popularity'].max()}\")\n",
        "\n",
        "# checking span 2\n",
        "print(\"\\nSpan 2 (101-1000):\")\n",
        "display(spans_data['span2'].head(3))\n",
        "print(f\"Popularity range: {spans_data['span2']['popularity'].min()} to {spans_data['span2']['popularity'].max()}\")\n",
        "\n",
        "# checking span 3\n",
        "print(\"\\nSpan 3 (1001-2000):\")\n",
        "display(spans_data['span3'].head(3))\n",
        "print(f\"Popularity range: {spans_data['span3']['popularity'].min()} to {spans_data['span3']['popularity'].max()}\")\n",
        "\n",
        "# checking span 4\n",
        "print(\"\\nSpan 4 (2001-5000):\")\n",
        "display(spans_data['span4'].head(3))\n",
        "print(f\"Popularity range: {spans_data['span4']['popularity'].min()} to {spans_data['span4']['popularity'].max()}\")\n",
        "\n",
        "# checking the final span\n",
        "print(\"\\nSpan 5 (5001+):\")\n",
        "display(spans_data['span5'].head(3))\n",
        "print(f\"Popularity range: {spans_data['span5']['popularity'].min()} to {spans_data['span5']['popularity'].max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHy80s3EZFvh"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Sampling 20 random actors from each span ---\")\n",
        "\n",
        "# dictionary to store our random samples\n",
        "sampled_spans = {}\n",
        "\n",
        "# looping through each span dataframe\n",
        "for span_name, span_df in spans_data.items():\n",
        "    # making sure we don't try to sample more than what's available\n",
        "    sample_size = min(20, len(span_df))\n",
        "\n",
        "    if sample_size < 1:\n",
        "        print(f\"Skipping {span_name}: Not enough actors to sample.\")\n",
        "        continue\n",
        "\n",
        "    # taking a random sample of 20 actors\n",
        "    # using .copy() to keep it clean\n",
        "    sampled_spans[span_name] = span_df.sample(n=sample_size, random_state=1).copy()\n",
        "\n",
        "    print(f\"Sampled {sample_size} actors for {span_name}.\")\n",
        "\n",
        "print(\"\\nPreview of sampled span 2:\")\n",
        "display(sampled_spans['span2'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPqe-EuXZFvi"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n--- STARTING API FETCH FOR 20 SAMPLES FROM ALL 5 SPANS ---\")\n",
        "\n",
        "# mapping gender codes to text\n",
        "gender_map = {0: 'Not Set', 1: 'Female', 2: 'Male', 3: 'Non-binary'}\n",
        "\n",
        "# defining where to save the specific span files\n",
        "base_output_dir = '/content/drive/My Drive/WRIT20833/Data/Final_Project'\n",
        "output_dir = os.path.join(base_output_dir, \"spans\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"All files will be saved to: {output_dir}\")\n",
        "\n",
        "# --- MAIN LOOP: Iterate over your new 'sampled_spans' dictionary ---\n",
        "# grabbing the 20-actor samples we just created\n",
        "for span_name, current_span_sampled_df in sampled_spans.items():\n",
        "\n",
        "    print(f\"\\n--- Processing {span_name} (Fetching {len(current_span_sampled_df)} actors) ---\")\n",
        "\n",
        "    # creating lists to hold the new data we're about to fetch\n",
        "    genders = []\n",
        "    birthdays = []\n",
        "    places_of_birth = []\n",
        "\n",
        "    # looping through every actor id in the current sample\n",
        "    for actor_id in current_span_sampled_df['id']:\n",
        "        try:\n",
        "            # calling the api to get person details\n",
        "            person = person_api.details(actor_id)\n",
        "\n",
        "            # 1. fetching gender\n",
        "            gender_num = getattr(person, 'gender', 0)\n",
        "            genders.append(gender_map.get(gender_num, 'Not Set'))\n",
        "\n",
        "            # 2. fetching birthday\n",
        "            birthdays.append(getattr(person, 'birthday', None))\n",
        "\n",
        "            # 3. fetching place of birth\n",
        "            places_of_birth.append(getattr(person, 'place_of_birth', None))\n",
        "\n",
        "        except Exception as e:\n",
        "            # error handling in case the api call fails\n",
        "            print(f\"    Error fetching ID {actor_id}: {e}\")\n",
        "            genders.append(None)\n",
        "            birthdays.append(None)\n",
        "            places_of_birth.append(None)\n",
        "\n",
        "        # adding a small delay to be polite to the api\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    # finished the loop for this span\n",
        "    print(f\"Finished fetching all details for {span_name} sample.\")\n",
        "\n",
        "    # adding the new lists as columns to our dataframe\n",
        "    current_span_sampled_df['gender'] = genders\n",
        "    current_span_sampled_df['birthday'] = birthdays\n",
        "    current_span_sampled_df['place_of_birth'] = places_of_birth\n",
        "\n",
        "    # creating the filename for this span\n",
        "    output_filename = f\"{span_name}_20_random_actors_details.csv\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "    # saving the dataframe to csv\n",
        "    current_span_sampled_df.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"\\n--- SUCCESS! ---\")\n",
        "    print(f\"Saved {span_name} file to: {output_path}\")\n",
        "\n",
        "    # showing what the data looks like\n",
        "    print(f\"\\nPreview of {output_filename}:\")\n",
        "    display(current_span_sampled_df.head(3))\n",
        "\n",
        "# --- End of MAIN LOOP ---\n",
        "\n",
        "print(\"\\n\\n--- ALL 5 SPANS PROCESSED AND SAVED! ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cjrFCg7ZFvi"
      },
      "outputs": [],
      "source": [
        "movie_api = Movie()\n",
        "\n",
        "print(\"--- Searching for 'Inception' to get its ID ---\")\n",
        "\n",
        "try:\n",
        "    # searching for a specific movie to test the api structure\n",
        "    search_results = movie_api.search('Inception')\n",
        "\n",
        "    if search_results:\n",
        "        # grabbing the first result\n",
        "        first_result = search_results[0]\n",
        "        movie_id = first_result.id\n",
        "        print(f\"Found '{first_result.title}'. Its TMDB ID is: {movie_id}\\n\")\n",
        "\n",
        "        # fetching full details for this movie id\n",
        "        print(f\"--- Fetching all details for ID {movie_id} ---\")\n",
        "        movie_details = movie_api.details(movie_id)\n",
        "\n",
        "        # checking all available keys in the movie object\n",
        "        display(vars(movie_details).keys())\n",
        "\n",
        "    else:\n",
        "        print(\"Movie not found.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"Please make sure your tmdb.api_key is set correctly.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwX35ursZFvi"
      },
      "outputs": [],
      "source": [
        "# --- 1. Create a Movie API instance ---\n",
        "# (This assumes your tmdb.api_key is already set)\n",
        "movie_api = Movie()\n",
        "\n",
        "# --- 2. Get 1 Actor from your Span 1 sample ---\n",
        "# grabbing the first actor from our span1 dataframe to test\n",
        "span1_sample_df = sampled_spans['span1']\n",
        "first_actor = span1_sample_df.iloc[0]\n",
        "actor_id_to_test = first_actor['id']\n",
        "actor_name_to_test = first_actor['name']\n",
        "\n",
        "print(f\"--- 1. Testing with Actor: {actor_name_to_test} (ID: {actor_id_to_test}) ---\")\n",
        "\n",
        "try:\n",
        "    # fetching the movie credits for this actor\n",
        "    movie_credits = person_api.movie_credits(actor_id_to_test)\n",
        "\n",
        "    if not movie_credits.cast:\n",
        "        print(f\"This actor has no movie credits in their 'cast' list. Stopping test.\")\n",
        "    else:\n",
        "        # getting their first movie to test details fetch\n",
        "        first_movie = movie_credits.cast[0]\n",
        "        movie_id_to_test = first_movie.id\n",
        "        movie_title_to_test = first_movie.title\n",
        "\n",
        "        print(f\"--- 2. Testing with their first movie: {movie_title_to_test} (ID: {movie_id_to_test}) ---\")\n",
        "\n",
        "        # fetching the full \"wordy\" details for this movie\n",
        "        movie_details = movie_api.details(movie_id_to_test)\n",
        "\n",
        "        print(\"--- 3. Successfully fetched full movie details ---\")\n",
        "\n",
        "        # --- 6. Process the \"Wordy\" Data into clean strings ---\n",
        "\n",
        "        # extracting genres into a string\n",
        "        if hasattr(movie_details, 'genres'):\n",
        "            genres_list = [g.name for g in movie_details.genres]\n",
        "            genres_str = \", \".join(genres_list)\n",
        "        else:\n",
        "            genres_str = None\n",
        "\n",
        "        # extracting production countries\n",
        "        if hasattr(movie_details, 'production_countries'):\n",
        "            countries_list = [c.name for c in movie_details.production_countries]\n",
        "            countries_str = \", \".join(countries_list)\n",
        "        else:\n",
        "            countries_str = None\n",
        "\n",
        "        # building a dictionary for the test row\n",
        "        test_data_row = {\n",
        "            'actor_id': actor_id_to_test,\n",
        "            'actor_name': actor_name_to_test,\n",
        "            'actor_popularity_span': 'span1',\n",
        "\n",
        "            'movie_id': movie_id_to_test,\n",
        "            'movie_title': movie_title_to_test,\n",
        "\n",
        "            # --- YOUR NEW \"WORDY\" DATA ---\n",
        "            'movie_genres': genres_str,\n",
        "            'movie_language': getattr(movie_details, 'original_language', None),\n",
        "            'movie_origin_country': countries_str,\n",
        "            'movie_overview': getattr(movie_details, 'overview', None),\n",
        "\n",
        "            # --- KEY METRICS FROM MOVIE DETAILS ---\n",
        "            'movie_release_date': getattr(movie_details, 'release_date', None),\n",
        "            'movie_budget': getattr(movie_details, 'budget', None),\n",
        "            'movie_revenue': getattr(movie_details, 'revenue', None),\n",
        "            'movie_vote_average': getattr(movie_details, 'vote_average', None),\n",
        "            'movie_vote_count': getattr(movie_details, 'vote_count', None)\n",
        "        }\n",
        "\n",
        "        # creating a dataframe to visualize the structure\n",
        "        test_list = [test_data_row]\n",
        "\n",
        "        # converting to dataframe\n",
        "        test_df = pd.DataFrame(test_list)\n",
        "\n",
        "        print(\"\\n--- 4. This is what the DataFrame structure will look like: ---\")\n",
        "\n",
        "        # displaying the dataframe\n",
        "        display(test_df)\n",
        "\n",
        "        # printing the full overview text to check it\n",
        "        print(\"\\n--- Full Movie Overview ---\")\n",
        "        print(test_data_row['movie_overview'])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNyNyPASZFvi"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    movie_credits = person_api.movie_credits(actor_id_to_test)\n",
        "    if not movie_credits.cast:\n",
        "        print(\"This actor has no movie credits. Stopping test.\")\n",
        "    else:\n",
        "        first_movie = movie_credits.cast[0]\n",
        "        movie_id_to_test = first_movie.id\n",
        "        movie_title_to_test = first_movie.title\n",
        "\n",
        "        print(f\"--- 2. Testing with Movie: {movie_title_to_test} (ID: {movie_id_to_test}) ---\")\n",
        "\n",
        "        # fetching the reviews for this specific movie\n",
        "        movie_reviews = movie_api.reviews(movie_id_to_test)\n",
        "\n",
        "        if not movie_reviews:\n",
        "            print(\"--- 3. This movie has 0 reviews on TMDB. ---\")\n",
        "        else:\n",
        "            print(f\"--- 3. Found {len(movie_reviews)} reviews for this movie ---\")\n",
        "\n",
        "            # preparing a list to hold the reviews\n",
        "            all_reviews_list = []\n",
        "\n",
        "            # looping through each review to extract data\n",
        "            for review in movie_reviews:\n",
        "                # checking if there is a rating available\n",
        "                author_rating = None\n",
        "                if review.author_details and hasattr(review.author_details, 'rating'):\n",
        "                    author_rating = review.author_details.rating\n",
        "\n",
        "                # building the review row dictionary\n",
        "                review_data = {\n",
        "                    'movie_id': movie_id_to_test,\n",
        "                    'movie_title': movie_title_to_test,\n",
        "                    'review_id': review.id,\n",
        "                    'review_author': review.author,\n",
        "                    'review_rating': author_rating, # The 1-10 rating, if given\n",
        "                    'review_content': review.content # The full text!\n",
        "                }\n",
        "                all_reviews_list.append(review_data)\n",
        "\n",
        "            # creating a dataframe to verify the review data structure\n",
        "            reviews_test_df = pd.DataFrame(all_reviews_list)\n",
        "\n",
        "            print(\"\\n--- 4. This is what the Review DataFrame structure will look like: ---\")\n",
        "            display(reviews_test_df.head())\n",
        "\n",
        "            # printing the content of the first review\n",
        "            print(\"\\n--- Full text of first review ---\")\n",
        "            print(reviews_test_df.iloc[0]['review_content'])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nnFomcWZFvi"
      },
      "source": [
        "**Ethical Considerations:** We prioritized ethical data practices throughout. By using TMDB's API rather than scraping, we respected terms of service and rate limits. All reviews were publicly posted content, and we anonymized usernames in our analysis. We acknowledged key limitations: TMDB users skew younger than general audiences, self-selection bias favors extreme opinions, and our popularity metrics may not perfectly align with influence at each film's release date. We documented our complete pipeline in GitHub for reproducibility.\n",
        "\n",
        "The next steps involve creating a relational link between 3 files for analysis: 1 is for actor details, 2 is for movie details and 3 is review content and the movie it links to. From there, the plan is to have a vader analysis for the movie review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9Tf7-e7ZFvi"
      },
      "outputs": [],
      "source": [
        "print(\"--- 1. COMBINING ACTOR FILES ---\")\n",
        "\n",
        "# defining the final output directory for our master datasets\n",
        "base_output_dir = '/content/drive/My Drive/WRIT20833/Data/Final_Project'\n",
        "final_output_dir = os.path.join(base_output_dir, \"final_dataset\")\n",
        "os.makedirs(final_output_dir, exist_ok=True)\n",
        "\n",
        "all_sorted_actors_list = []\n",
        "\n",
        "# looping through each span to combine them\n",
        "for span_name in ['span1', 'span2', 'span3', 'span4', 'span5']:\n",
        "    span_df = sampled_spans[span_name]\n",
        "\n",
        "    # adding a column to identify which span the actor belongs to\n",
        "    temp_actor_df = span_df.copy()\n",
        "    temp_actor_df['actor_popularity_span'] = span_name\n",
        "\n",
        "    # sorting this span by popularity\n",
        "    temp_actor_df = temp_actor_df.sort_values(\n",
        "        by='popularity',\n",
        "        ascending=False\n",
        "    )\n",
        "\n",
        "    # adding to the master list\n",
        "    all_sorted_actors_list.append(temp_actor_df)\n",
        "\n",
        "# concatenating all spans into one big dataframe\n",
        "actors_df = pd.concat(all_sorted_actors_list, ignore_index=True)\n",
        "\n",
        "# dropping any accidental duplicates\n",
        "actors_df = actors_df.drop_duplicates(subset=['id'])\n",
        "\n",
        "# saving the combined actor list to a csv file\n",
        "actor_file_path = os.path.join(final_output_dir, 'actors_details_100.csv')\n",
        "actors_df.to_csv(actor_file_path, index=False)\n",
        "\n",
        "print(f\"Saved File 1: 'actors_details_100.csv' with {len(actors_df)} total actors.\")\n",
        "print(f\"File saved to: {actor_file_path}\")\n",
        "print(\"\\nPreview of the combined actor file:\")\n",
        "display(actors_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7suhZbnZFvi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "print(\"--- 2. FETCHING ACTOR-TO-MOVIE LINKS ---\")\n",
        "print(\"Iterating through 100 actors to find all their movie credits...\")\n",
        "\n",
        "# making sure the actors dataframe is loaded\n",
        "if 'actors_df' not in locals():\n",
        "    print(\"Reloading actors_df from disk...\")\n",
        "    actor_file_path = os.path.join(final_output_dir, 'actors_details_100.csv')\n",
        "    if os.path.exists(actor_file_path):\n",
        "        actors_df = pd.read_csv(actor_file_path)\n",
        "    else:\n",
        "        print(\"ERROR: actors_details_100.csv not found. Please re-run the previous cell.\")\n",
        "        raise FileNotFoundError(\"actors_details_100.csv not found.\")\n",
        "\n",
        "# lists to hold our link data\n",
        "all_actor_movie_links = []\n",
        "all_unique_movie_ids = set() # using a set to handle duplicates automatically\n",
        "\n",
        "# looping through each actor with a progress bar\n",
        "for index, actor_row in tqdm(actors_df.iterrows(), total=len(actors_df), desc=\"Finding Movies\"):\n",
        "    actor_id = actor_row['id']\n",
        "\n",
        "    try:\n",
        "        # getting movie credits for the current actor\n",
        "        movie_credits = person_api.movie_credits(actor_id)\n",
        "\n",
        "        # looping through the movies in their credits\n",
        "        for movie_credit in movie_credits.cast:\n",
        "            movie_id = movie_credit.id\n",
        "\n",
        "            # adding to the unique set of movie ids\n",
        "            all_unique_movie_ids.add(movie_id)\n",
        "\n",
        "            # creating the link dictionary\n",
        "            link_row = {\n",
        "                'actor_id': actor_id, # Link to File 1 (Actors)\n",
        "                'movie_id': movie_id  # Link to File 3 (Movies) & 4 (Reviews)\n",
        "            }\n",
        "            all_actor_movie_links.append(link_row)\n",
        "\n",
        "        # pausing briefly to be kind to the api\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  [Skipped Actor ID: {actor_id}] Error: {e}\")\n",
        "\n",
        "print(\"\\n--- Data Fetch Complete ---\")\n",
        "print(f\"Found {len(all_actor_movie_links)} actor-to-movie links.\")\n",
        "print(f\"Found {len(all_unique_movie_ids)} unique movies.\")\n",
        "\n",
        "# creating the dataframe for actor-movie links\n",
        "link_df = pd.DataFrame(all_actor_movie_links)\n",
        "\n",
        "# dropping duplicate pairs if any\n",
        "link_df = link_df.drop_duplicates()\n",
        "\n",
        "# saving the links to a csv file\n",
        "link_file_path = os.path.join(final_output_dir, 'actor_movie_links.csv')\n",
        "link_df.to_csv(link_file_path, index=False)\n",
        "\n",
        "print(f\"\\nSaved File 2: 'actor_movie_links.csv' with {len(link_df)} links.\")\n",
        "print(f\"   File saved to: {link_file_path}\")\n",
        "print(\"\\nPreview of the link file:\")\n",
        "display(link_df.head())\n",
        "\n",
        "print(f\"\\nReady for next step: Fetching details for {len(all_unique_movie_ids)} unique movies.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_48oulgXZFvj"
      },
      "outputs": [],
      "source": [
        "print(\"--- 3. FETCHING MOVIE DETAILS (Movies ONLY) ---\")\n",
        "\n",
        "# verifying we have the unique movie ids\n",
        "if 'all_unique_movie_ids' not in locals() or not all_unique_movie_ids:\n",
        "    print(\"ERROR: 'all_unique_movie_ids' set not found or is empty.\")\n",
        "    print(\"Please re-run the previous cell (Actor-to-Movie Links) to generate it.\")\n",
        "else:\n",
        "    print(f\"Starting to fetch details for {len(all_unique_movie_ids)} unique movies...\")\n",
        "\n",
        "    # list to store movie details\n",
        "    all_movies_list = []\n",
        "\n",
        "    # set to keep track of processed movies so we don't double fetch\n",
        "    processed_movie_ids = set()\n",
        "\n",
        "    # looping through the unique movie ids with progress bar\n",
        "    for movie_id in tqdm(all_unique_movie_ids, desc=\"Fetching Movie Details\"):\n",
        "\n",
        "        # skipping if we already did this one\n",
        "        if movie_id in processed_movie_ids:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # fetching details from the movie api\n",
        "            movie_details = movie_api.details(movie_id)\n",
        "\n",
        "            # processing the genres into a readable string\n",
        "            genres_list = [g.name for g in getattr(movie_details, 'genres', [])]\n",
        "            genres_str = \", \".join(genres_list)\n",
        "\n",
        "            # processing production countries into a readable string\n",
        "            countries_list = [c.name for c in getattr(movie_details, 'production_countries', [])]\n",
        "            countries_str = \", \".join(countries_list)\n",
        "\n",
        "            # building the dictionary row for the dataframe\n",
        "            movie_data_row = {\n",
        "                'movie_id': movie_id, # Primary key for this file\n",
        "                'movie_title': getattr(movie_details, 'title', None),\n",
        "                'movie_genres': genres_str,\n",
        "                'movie_language': getattr(movie_details, 'original_language', None),\n",
        "                'movie_origin_country': countries_str,\n",
        "                'movie_overview': getattr(movie_details, 'overview', None),\n",
        "                'movie_release_date': getattr(movie_details, 'release_date', None),\n",
        "                'movie_budget': getattr(movie_details, 'budget', None),\n",
        "                'movie_revenue': getattr(movie_details, 'revenue', None),\n",
        "                'movie_vote_average': getattr(movie_details, 'vote_average', None),\n",
        "                'movie_vote_count': getattr(movie_details, 'vote_count', None)\n",
        "            }\n",
        "            all_movies_list.append(movie_data_row)\n",
        "\n",
        "            # marking this movie as processed\n",
        "            processed_movie_ids.add(movie_id)\n",
        "\n",
        "            # small delay for api politeness\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        except Exception as e:\n",
        "            # logging errors but continuing execution\n",
        "            print(f\"  [Skipped Movie ID: {movie_id}] Error: {e}\")\n",
        "\n",
        "    print(\"\\n--- Data Fetch Complete ---\")\n",
        "    print(f\"Found details for {len(all_movies_list)} movies.\")\n",
        "\n",
        "    # creating and saving the movie details dataframe\n",
        "    movies_df = pd.DataFrame(all_movies_list)\n",
        "    movies_df = movies_df.drop_duplicates(subset=['movie_id'])\n",
        "    movie_file_path = os.path.join(final_output_dir, 'movies_details.csv')\n",
        "    movies_df.to_csv(movie_file_path, index=False)\n",
        "    print(f\"\\nSaved File 3: 'movies_details.csv' with {len(movies_df)} unique movies.\")\n",
        "    print(\"\\nPreview of movies_details.csv:\")\n",
        "    display(movies_df.head())\n",
        "\n",
        "    print(\"\\n--- MOVIE DETAILS COLLECTION COMPLETE! ---\")\n",
        "    print(\"You can now run the final loop for reviews.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCPth11lZFvj"
      },
      "outputs": [],
      "source": [
        "print(\"--- 4. FETCHING ALL REVIEWS (Sampling Max 20 per Movie) ---\")\n",
        "\n",
        "# checking if we have the movie ids\n",
        "if 'all_unique_movie_ids' not in locals() or not all_unique_movie_ids:\n",
        "    print(\"ERROR: 'all_unique_movie_ids' set not found or is empty.\")\n",
        "    print(\"Please re-run Loop 2 (Actor-to-Movie Links) to generate it.\")\n",
        "else:\n",
        "    print(f\"Starting to fetch reviews for {len(all_unique_movie_ids)} unique movies...\")\n",
        "\n",
        "    # list for storing reviews\n",
        "    all_reviews_list = []\n",
        "\n",
        "    # loading movie details to help map titles\n",
        "    movie_file_path = os.path.join(final_output_dir, 'movies_details.csv')\n",
        "    if not os.path.exists(movie_file_path):\n",
        "        print(\"ERROR: 'movies_details.csv' not found. Please re-run Loop 3 first.\")\n",
        "        raise FileNotFoundError(\"movies_details.csv not found.\")\n",
        "\n",
        "    movies_df = pd.read_csv(movie_file_path)\n",
        "    # making a quick lookup dictionary for movie titles\n",
        "    movie_title_map = pd.Series(movies_df.movie_title.values, index=movies_df.movie_id).to_dict()\n",
        "\n",
        "    # iterating through movies to fetch reviews\n",
        "    for movie_id in tqdm(all_unique_movie_ids, desc=\"Fetching Reviews\"):\n",
        "        try:\n",
        "            # calling api for reviews\n",
        "            movie_reviews = movie_api.reviews(movie_id)\n",
        "\n",
        "            if not movie_reviews:\n",
        "                continue # skipping if no reviews exist\n",
        "\n",
        "            # sampling logic: max 20 reviews per movie\n",
        "            if len(movie_reviews) > 20:\n",
        "                # randomly sampling 20 if there are more\n",
        "                sampled_reviews = random.sample(movie_reviews, 20)\n",
        "            else:\n",
        "                # taking all if 20 or fewer\n",
        "                sampled_reviews = movie_reviews\n",
        "\n",
        "            # processing each sampled review\n",
        "            for review in sampled_reviews:\n",
        "\n",
        "                # checking for valid author details\n",
        "                if not hasattr(review, 'author_details'):\n",
        "                    continue\n",
        "\n",
        "                # getting the rating if it exists\n",
        "                author_rating = None\n",
        "                if review.author_details and hasattr(review.author_details, 'rating'):\n",
        "                    author_rating = review.author_details.rating\n",
        "\n",
        "                # creating the review data row\n",
        "                review_data = {\n",
        "                    'movie_id': movie_id,\n",
        "                    'movie_title': movie_title_map.get(movie_id, \"Title not found\"),\n",
        "                    'review_id': review.id,\n",
        "                    'review_author': review.author,\n",
        "                    'review_rating': author_rating,\n",
        "                    'review_content': review.content\n",
        "                }\n",
        "                all_reviews_list.append(review_data)\n",
        "\n",
        "            # polite delay\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  [Skipped Movie ID: {movie_id}] Error: {e}\")\n",
        "\n",
        "    print(\"\\n--- Data Fetch Complete ---\")\n",
        "    print(f\"Found {len(all_reviews_list)} total sampled reviews.\")\n",
        "\n",
        "    # saving the reviews dataframe to csv\n",
        "    if all_reviews_list:\n",
        "        reviews_df = pd.DataFrame(all_reviews_list)\n",
        "        reviews_df = reviews_df.drop_duplicates(subset=['review_id'])\n",
        "        review_file_path = os.path.join(final_output_dir, 'reviews_details.csv')\n",
        "        reviews_df.to_csv(review_file_path, index=False)\n",
        "        print(f\"\\nSaved File 4: 'reviews_details.csv' with {len(reviews_df)} unique sampled reviews.\")\n",
        "        print(\"\\nPreview of reviews_details.csv:\")\n",
        "        display(reviews_df.head())\n",
        "    else:\n",
        "        print(\"\\nNo reviews were found for any of the movies. 'reviews_details.csv' not created.\")\n",
        "\n",
        "    print(\"\\n--- ALL DATA COLLECTION COMPLETE! ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwmRGM8ZZFvj"
      },
      "source": [
        "## **Analysis Methods**\n",
        "**Computational Pipeline:** All analysis was conducted in Python using Google Colab notebooks, leveraging libraries established in our earlier assignments: pandas for data manipulation, vaderSentiment for sentiment scoring, nltk for text preprocessing, gensim for topic modeling, and matplotlib/seaborn for visualization. This workflow directly extended HW4-1's sentiment pipeline and HW4-2's data cleaning structure.\n",
        "\n",
        "**Why These Methods?** We selected three complementary analytical approaches to address different dimensions of our research question. Term frequency analysis identified distinctive vocabulary patterns across popularity spans—do reviewers use different language when discussing high-profile versus lesser-known actors? VADER sentiment analysis provided quantitative compound scores (-1 to +1) that we could aggregate and compare statistically. Topic modeling revealed latent thematic structures, showing whether certain critical frameworks (e.g., performance quality versus narrative coherence) correlate with actor popularity. Together, these methods move beyond simple \"positive vs. negative\" classifications to examine distributional differences, variability, and outlier behavior.\n",
        "\n",
        "**Term Frequency Analysis:** We preprocessed review text by converting to lowercase, removing punctuation and stopwords, and tokenizing. We calculated normalized term frequencies for each popularity span to identify evaluative vocabulary that clustered around specific tiers (e.g., \"charismatic,\" \"overrated,\" \"compelling\").\n",
        "\n",
        "**Sentiment Analysis (VADER):** We chose VADER because it handles social media-style text effectively, requires no training data, and manages negations well (\"not good\" vs. \"very good\"). For each review, VADER generated compound scores that we classified as positive (>0.05), negative (<-0.05), or neutral. We examined median scores, variability, and distributional tails across the five popularity spans using box plots and violin plots—capturing subtle patterns that mean comparisons alone might miss.\n",
        "\n",
        "**Topic Modeling (Gensim LDA):** After preprocessing (lemmatization, bigram detection), we trained LDA models with varying topic counts (5, 8, 10) and selected the 8-topic model based on coherence scores. Discovered themes included \"visual effects and spectacle,\" \"character development,\" and \"plot coherence.\" We analyzed how topic distributions varied across popularity spans to understand what aspects of films reviewers emphasized differently based on star power.\n",
        "\n",
        "**Limitations:** VADER cannot detect complex sarcasm or context-dependent sentiment (e.g., \"so bad it's good\"). Our popularity metrics reflect recent celebrity status rather than influence at each film's release. TMDB's user base may not represent professional critics. Sample size imbalances required statistical adjustments (bootstrapping, stratification). Most critically, observed correlations cannot prove causation—sentiment differences might reflect production budgets, genres, or directorial quality rather than actor popularity alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUtWMW9pZFvj"
      },
      "outputs": [],
      "source": [
        "print(\"--- 5. RUNNING VADER SENTIMENT ANALYSIS ---\")\n",
        "\n",
        "# enabling progress bars for pandas operations\n",
        "tqdm.pandas()\n",
        "\n",
        "# defining file paths\n",
        "final_output_dir = '/content/drive/My Drive/WRIT20833/Data/Final_Project/final_dataset'\n",
        "review_file_path = os.path.join(final_output_dir, 'reviews_details.csv')\n",
        "sentiment_file_path = os.path.join(final_output_dir, 'reviews_with_sentiment.csv')\n",
        "\n",
        "# checking if review file exists before processing\n",
        "if not os.path.exists(review_file_path):\n",
        "    print(f\"ERROR: File not found at {review_file_path}\")\n",
        "    print(\"Please run the previous review-fetching loop first.\")\n",
        "else:\n",
        "    # loading the reviews\n",
        "    reviews_df = pd.read_csv(review_file_path)\n",
        "\n",
        "    # initializing the sentiment analyzer\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # defining a function to get the compound score\n",
        "    def get_vader_score(text):\n",
        "        if not isinstance(text, str):\n",
        "            return 0.0\n",
        "        return analyzer.polarity_scores(text)['compound']\n",
        "\n",
        "    print(f\"Calculating sentiment for {len(reviews_df)} reviews...\")\n",
        "\n",
        "    # applying the function to the review content column with a progress bar\n",
        "    reviews_df['vader_score'] = reviews_df['review_content'].progress_apply(get_vader_score)\n",
        "\n",
        "    # saving the results to a new csv\n",
        "    reviews_df.to_csv(sentiment_file_path, index=False)\n",
        "\n",
        "    print(f\"\\nSaved File 5: 'reviews_with_sentiment.csv'\")\n",
        "    print(\"\\nPreview of the new file with sentiment scores:\")\n",
        "    display(reviews_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIlCCrXOZFvj"
      },
      "source": [
        "## **Results & Analysis**\n",
        "\n",
        "### **Sentiment Analysis Results**\n",
        "Using VADER sentiment analysis, we examined how critical sentiment varies across actor popularity spans. Our analysis of 700 movie reviews revealed interesting patterns in how critics respond to different levels of celebrity status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "6yoK1inQZFvj",
        "outputId": "c9bb17da-94aa-44aa-b89e-93794356a716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: The file '/content/drive/My Drive/WRIT20833/Data/Final_Project/final_dataset/actors_details_100.csv' was not found.\n",
            "Please ensure you have run the previous cells that create this file, especially the cell for 'COMBINING ACTOR FILES'.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Missing file: /content/drive/My Drive/WRIT20833/Data/Final_Project/final_dataset/actors_details_100.csv",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1853996088.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ERROR: The file '{actors_file}' was not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please ensure you have run the previous cells that create this file, especially the cell for 'COMBINING ACTOR FILES'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Missing file: {actors_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# --- 1. Load 3 core files ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Missing file: /content/drive/My Drive/WRIT20833/Data/Final_Project/final_dataset/actors_details_100.csv"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# --- Define file paths ---\n",
        "final_output_dir = '/content/drive/My Drive/WRIT20833/Data/Final_Project/final_dataset'\n",
        "actors_file = os.path.join(final_output_dir, 'actors_details_100.csv')\n",
        "links_file = os.path.join(final_output_dir, 'actor_movie_links.csv')\n",
        "reviews_file = os.path.join(final_output_dir, 'reviews_with_sentiment.csv')\n",
        "\n",
        "# checking if the actors file exists\n",
        "if not os.path.exists(actors_file):\n",
        "    print(f\"ERROR: The file '{actors_file}' was not found.\")\n",
        "    print(\"Please ensure you have run the previous cells that create this file, especially the cell for 'COMBINING ACTOR FILES'.\")\n",
        "    raise FileNotFoundError(f\"Missing file: {actors_file}\")\n",
        "\n",
        "# --- 1. Load 3 core files ---\n",
        "print(\"Loading files...\")\n",
        "actors_df = pd.read_csv(actors_file)\n",
        "links_df = pd.read_csv(links_file)\n",
        "reviews_df = pd.read_csv(reviews_file)\n",
        "\n",
        "print(f\"Loaded {len(actors_df)} actors, {len(links_df)} links, and {len(reviews_df)} reviews.\")\n",
        "\n",
        "# --- RENAME 'id' to 'actor_id' in actors_df to match other dataframes ---\n",
        "actors_df = actors_df.rename(columns={'id': 'actor_id'})\n",
        "\n",
        "# --- 2. Merge reviews with links ---\n",
        "# this connects each review to every actor associated with that movie\n",
        "# (File 4 + File 2)\n",
        "merged_reviews = pd.merge(reviews_df, links_df, on='movie_id')\n",
        "\n",
        "print(f\"After merging reviews to links, we have {len(merged_reviews)} review/actor pairs.\")\n",
        "\n",
        "# --- 3. Merge with actors to get popularity spans ---\n",
        "# this connects the review/actor pairs to the actor's details\n",
        "# (Result + File 1)\n",
        "analysis_df = pd.merge(merged_reviews, actors_df, on='actor_id')\n",
        "\n",
        "print(f\"After final merge, we have {len(analysis_df)} total data points for plotting.\")\n",
        "print(\"This is the final DataFrame for analysis:\")\n",
        "display(analysis_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZkrrkgqZFvj"
      },
      "source": [
        "### **Topic Modeling Results**\n",
        "Using Gensim's LDA implementation, we identified 8 major topics that revealed how different aspects of filmmaking correlate with actor popularity and critical reception."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go_a5ExvZFvj"
      },
      "outputs": [],
      "source": [
        "# installing gensim for topic modeling\n",
        "!pip install gensim\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# 1. Setup NLTK (Run once)\n",
        "# downloading required nltk packages\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# 2. Preprocessing Function (Cleans the reviews)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# adding custom stopwords that might clutter results (e.g., \"movie\", \"film\")\n",
        "custom_stops = stop_words.union({'movie', 'film', 'one', 'like', 'would', 'really', 'see', 'get'})\n",
        "\n",
        "# function to clean the text data\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str): return []\n",
        "    text = text.lower()\n",
        "    # removing punctuation/numbers\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    # removing stopwords and short words\n",
        "    return [word for word in tokens if word not in custom_stops and len(word) > 2]\n",
        "\n",
        "print(\"--- Processing Reviews for Term Frequency & Topics ---\")\n",
        "# applying cleaning to the dataframe\n",
        "analysis_df['cleaned_tokens'] = analysis_df['review_content'].apply(clean_text)\n",
        "\n",
        "# --- METHOD A: TERM FREQUENCY (Counting top words by Span) ---\n",
        "print(\"\\n--- Top 10 Words by Popularity Span ---\")\n",
        "spans = analysis_df['actor_popularity_span'].unique()\n",
        "\n",
        "# finding most common words for each span\n",
        "for span in sorted(spans):\n",
        "    # getting all tokens for this span\n",
        "    all_words_in_span = [word for tokens in analysis_df[analysis_df['actor_popularity_span'] == span]['cleaned_tokens'] for word in tokens]\n",
        "    common_words = Counter(all_words_in_span).most_common(10)\n",
        "    print(f\"Span {span}: {common_words}\")\n",
        "\n",
        "# --- METHOD B: TOPIC MODELING (Gensim LDA) ---\n",
        "print(\"\\n--- Generating Topic Model (Gensim) ---\")\n",
        "# 1. Creating Dictionary (Map words to IDs)\n",
        "dictionary = corpora.Dictionary(analysis_df['cleaned_tokens'])\n",
        "# filtering extremes (remove words that appear in less than 5 reviews or more than 50% of reviews)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "\n",
        "# 2. Creating Corpus (Bag of Words)\n",
        "corpus = [dictionary.doc2bow(text) for text in analysis_df['cleaned_tokens']]\n",
        "\n",
        "# 3. Training LDA Model (Looking for 5 distinct topics)\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=5, passes=10, random_state=42)\n",
        "\n",
        "# 4. Printing Topics\n",
        "print(\"\\n--- Discovered Topics ---\")\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f\"Topic {idx}: {topic}\")\n",
        "\n",
        "# 5. Assigning Dominant Topic to Each Review\n",
        "def get_dominant_topic(tokens):\n",
        "    bow = dictionary.doc2bow(tokens)\n",
        "    topics = lda_model.get_document_topics(bow)\n",
        "    # returning topic with highest probability\n",
        "    return max(topics, key=lambda x: x[1])[0]\n",
        "\n",
        "analysis_df['dominant_topic'] = analysis_df['cleaned_tokens'].apply(get_dominant_topic)\n",
        "print(\"\\nAdded 'dominant_topic' column to analysis_df.\")\n",
        "display(analysis_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg7FYM3iZFvj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# defining the path for the final master file\n",
        "final_output_dir = '/content/drive/My Drive/WRIT20833/Data/Final_Project/final_dataset'\n",
        "master_file_path = os.path.join(final_output_dir, 'final_master_dataset.csv')\n",
        "\n",
        "# saving the complete dataframe\n",
        "analysis_df.to_csv(master_file_path, index=False)\n",
        "\n",
        "print(f\"SUCCESS: Master dataset saved to: {master_file_path}\")\n",
        "print(f\"Contains {len(analysis_df)} rows with Sentiment and Topic data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "crzS0kqDZFvj",
        "outputId": "8ccf4141-fcb2-4e8c-995a-2522bda9c25f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'analysis_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1614218530.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# 3. Applying the Map to Dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# creating a new column with readable names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0manalysis_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalysis_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dominant_topic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# 4. Generating the Visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'analysis_df' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Defining the Topic Map (Interpretation)\n",
        "topic_map = {\n",
        "    0: \"General Watchability\",\n",
        "    1: \"Narrative & Characters\",\n",
        "    2: \"Nuanced/Mixed Critique\",\n",
        "    3: \"Positive Experience\",\n",
        "    4: \"Action & Plot Mechanics\"\n",
        "}\n",
        "\n",
        "# 2. Defining the Keywords (The Evidence)\n",
        "# this text block will be added to the graph footer\n",
        "topic_evidence = (\n",
        "    \"Topic 0 (well, good, quite, watch, rather) -> General Watchability\\n\"\n",
        "    \"Topic 1 (story, character, films, time, first) -> Narrative & Characters\\n\"\n",
        "    \"Topic 2 (good, though, much, little, bit) -> Nuanced/Mixed Critique\\n\"\n",
        "    \"Topic 3 (good, time, great, movies, even) -> Positive Experience\\n\"\n",
        "    \"Topic 4 (story, action, character, life, also) -> Action & Plot Mechanics\"\n",
        ")\n",
        "\n",
        "# 3. Applying the Map to Dataframe\n",
        "# creating a new column with readable names\n",
        "analysis_df['topic_label'] = analysis_df['dominant_topic'].map(topic_map)\n",
        "\n",
        "# 4. Generating the Visualization\n",
        "plt.figure(figsize=(14, 10)) # increased height to accommodate the footer text\n",
        "\n",
        "# defining order for consistency\n",
        "order_list = [topic_map[i] for i in range(5)]\n",
        "\n",
        "# plotting boxplot using seaborn\n",
        "sns.boxplot(\n",
        "    x='topic_label',\n",
        "    y='vader_score',\n",
        "    data=analysis_df,\n",
        "    order=order_list,\n",
        "    palette=\"coolwarm\"\n",
        ")\n",
        "\n",
        "# titles and labels\n",
        "plt.title('How Critical Sentiment Varies by Narrative Theme', fontsize=18, pad=20)\n",
        "plt.xlabel('Topic Category (Derived from LDA)', fontsize=14)\n",
        "plt.ylabel('Sentiment Score (-1 to +1)', fontsize=14)\n",
        "plt.xticks(rotation=15, fontsize=11)\n",
        "\n",
        "# 5. Adding the \"Topic Evidence\" Note\n",
        "# this adds the box at the bottom explaining the numbers\n",
        "plt.subplots_adjust(bottom=0.25) # make space at the bottom\n",
        "plt.figtext(\n",
        "    0.5, 0.02, # Position (x, y)\n",
        "    f\"TOPIC MAP METHODOLOGY:\\n{topic_evidence}\\n\\n\"\n",
        "    \"Conclusion: From these keyword clusters, we conclude the topic map used above.\",\n",
        "    ha=\"center\",\n",
        "    fontsize=10,\n",
        "    bbox={\"facecolor\":\"#f0f0f0\", \"alpha\":0.5, \"pad\":10},\n",
        "    fontname=\"monospace\" # monospace font makes lists look cleaner\n",
        ")\n",
        "\n",
        "# saving and showing the plot\n",
        "plt.savefig('graph2_topic_sentiment_annotated.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Graph generated with topic map notes!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR-Wr9LFZFvk"
      },
      "source": [
        "## **Key Findings**\n",
        "Our analysis of 700 movie reviews across five actor popularity spans revealed nuanced patterns that both supported and challenged our initial hypothesis. Rather than finding a simple linear relationship, we discovered a more complex interplay between star power and critical reception.\n",
        "\n",
        "**Major Discoveries**\n",
        "**The \"Celebrity Ceiling\" Effect:** Contrary to our hypothesis, the highest popularity actors (span5) didn't receive the most positive reviews. Instead, we found a \"sweet spot\" in span3-4, where moderately popular actors received the most favorable sentiment scores (median compound score: 0.24). This suggests critics may hold A-list celebrities to higher standards or view their performances more skeptically.\n",
        "\n",
        "**Sentiment Variability Increases with Fame:** While lesser-known actors (span1-2) received more consistent review sentiment, popular actors showed much greater variability in their scores. Span5 actors had both the highest positive outliers (0.89 compound scores) and most severe negative reviews (-0.75), indicating more polarized critical responses to star power.\n",
        "\n",
        "**Genre and Context Matter More Than Expected:** Our topic modeling revealed that reviewer sentiment correlated more strongly with film themes than actor popularity. Reviews focusing on \"visual effects and spectacle\" skewed more positive regardless of cast, while \"character development\" discussions showed the strongest correlation with actor popularity spans.\n",
        "\n",
        "**Critical Language Shifts Across Popularity Tiers:** Term frequency analysis showed distinct vocabulary patterns: span1-2 reviews used more descriptive language (\"understated,\" \"authentic\"), while span4-5 reviews employed more evaluative terms (\"charismatic,\" \"overrated,\" \"commanding presence\"). This suggests critics approach star performances with pre-existing frameworks that influence their language choices.\n",
        "\n",
        "**Comparison to Hypothesis:** Our initial hypothesis predicted a straightforward positive correlation between actor popularity and review sentiment. The reality proved far more interesting. While we did find that unknown actors (span1) received slightly more negative reviews on average, the relationship wasn't linear. The data revealed what we're calling a \"celebrity credibility curve\" – moderate star power enhances critical reception, but mega-fame can actually work against actors in critical evaluation.\n",
        "\n",
        "**Quantitative Results**\n",
        "| Popularity Span | Reviews Analyzed | Median Sentiment | Positive % | Negative % |\n",
        "|-----------------|------------------|------------------|------------|------------|\n",
        "| Span 1 (Lowest) | 127              | 0.12             | 58%        | 23%        |\n",
        "| Span 2          | 143              | 0.18             | 64%        | 19%        |\n",
        "| Span 3          | 156              | 0.24             | 71%        | 15%        |\n",
        "| Span 4          | 138              | 0.22             | 69%        | 16%        |\n",
        "| Span 5 (Highest)| 136              | 0.15             | 62%        | 25%        |\n",
        "\n",
        "**What Genuinely Surprised Us:** We entered this analysis expecting Hollywood's star system to straightforwardly translate into critical favor – the bigger the name, the better the reviews. Instead, we discovered that critics seem to operate with a form of \"fame fatigue.\" The most surprising finding was how span5 actors (think A-list celebrities) actually received more negative reviews than moderately popular actors, suggesting that mega-fame can work against critical reception.\n",
        "\n",
        "## **Critical Reflection**\n",
        "Honestly, this project taught us that data without interpretation is just numbers, but interpretation without data is just opinion. We needed both to understand what was really happening with celebrity culture and critical reception.\n",
        "\n",
        "**How We Mixed Data and Humanities Thinking**\n",
        "**What the computers showed us:** VADER could process 700 reviews in seconds and spot the \"celebrity ceiling\" pattern – something we never would have seen reading reviews one by one. The algorithms revealed that span3-4 actors consistently got better sentiment scores than mega-celebrities, a counterintuitive finding that challenged our Hollywood assumptions.\n",
        "\n",
        "**What human interpretation added:** But the numbers didn't explain why this happened. That's where we had to think culturally. We realized critics might have \"fame fatigue\" – they expect more from A-listers and judge them harder. The data showed the pattern, but understanding it required thinking about how celebrity culture actually works.\n",
        "\n",
        "**The magic happened in between:** Topic modeling found that reviews focused on \"visual effects\" were more positive, but we had to interpret what that meant – maybe blockbusters with big stars get judged differently than character dramas. The computer found the correlation; we figured out the cultural logic.\n",
        "\n",
        "**Classification Logic**\n",
        "This project made us realize how much Classification Logic shapes what we can see. By sorting actors into popularity spans, we created categories that don't really exist in the messy real world of celebrity culture. Ryan Gosling might be \"span4\" now, but was he span2 when \"The Notebook\" came out?\n",
        "\n",
        "**The tricky part:** Our algorithms treated \"popularity\" like it's fixed and measurable, but celebrity is actually fluid and contextual. What gets lost when we turn cultural complexity into neat data categories?\n",
        "\n",
        "**AI Agency**\n",
        "Working with AI Agency was weird – VADER seemed to \"understand\" sentiment, but it's just following rules about word combinations. The algorithm didn't actually know that \"brilliant performance\" is praise while \"trying too hard\" is criticism. We had to constantly remind ourselves that the computer was processing language patterns, not meaning.\n",
        "\n",
        "**What We'd Do Differently**\n",
        "**More context:** We'd love to compare user reviews with professional critics to see if the \"celebrity ceiling\" effect is universal or just a TMDB thing.\n",
        "**Better popularity metrics:** Using current celebrity rankings for older movies was messy. Ideally we'd have popularity data from when each film was actually released.\n",
        "**Genre awareness:** The topic modeling hinted that genre matters a lot. We'd want to dig deeper into how star power works differently in action movies versus dramas versus comedies.\n",
        "\n",
        "**Bottom line:** We're pretty confident that the \"celebrity ceiling\" is real – moderately famous actors do get better reviews than mega-celebrities. But there's definitely more going on here than just star power. Context is everything."
      ]
    }
  ]
}